{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before We Start\n",
    "\n",
    "If you're running this notebook locally, I suggest you install [Jupyter notebook extensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions) first. You should [enable](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html#enabling-disabling-extensions) Python Markdown. If you see the \"untrusted\" question mark above üëÜ, use **File -> Trust Notebook** in the menu.\n",
    "\n",
    "If you don't want to bother with the installation, this notebook is also available on binder. \n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/doctornerdis/data-incubator-teaching-demo/master?filepath=teaching-demo.ipynb)\n",
    "\n",
    "For the purposes of this demonstration, I'm suppressing some types of warnings to minimize distraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ReduceDistraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Brief\n",
    "\n",
    "Since I'm a teacher, I always begin an assignment with the objectives and evaluation criteria front and center. In order to provide better context for this presentation, I'm starting with the email I received from Pascal. It outlines the major points I've been asked to discuss today and the expectations for my presentation. \n",
    "\n",
    "> We want to see how well you're able to explain topics in statistics and data science. Write a short Jupyter notebook (and put it on github) covering these topics:\n",
    ">\n",
    "> 1. Hypothesis Testing: Let's talk about t-tests, p-values.¬† How are they related? What is it telling you?¬† How does it relate to precision-recall? What are the underlying assumptions?\n",
    "> 2. Bayesian posterior inference: Explain Bayes' Rule.¬† Write some code to actually perform posterior sampling.¬† Work out an example using conjugate priors. How does this compare with hypothesis testing?¬† What are the underlying assumptions?\n",
    ">\n",
    "> Be prepared to give a short mock \"lecture\" (30 minutes) about these two topics with your prepared Jupyter notebooks.¬† The Jupyter notebooks are more meant to be notes for yourself and visual aids for your lecture. We'll be looking for\n",
    ">\n",
    "> 1. How well you present: remember that this material should be approachable, applied, and not just a series of formulas\n",
    "> 2. How well you understand these topics in depth (the mathematics, the underlying assumptions behind ideas, etc ...)\n",
    "\n",
    "Based on Pascal's email, I've created an example that touches on both hypothesis testing and Bayesian inference. It's based on my time in graduate school and my love of good food. üçï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Story: New Haven Apizza\n",
    "\n",
    "I did my graduate studies in New Haven, CT. One thing that New Haveners take very seriously is \"apizza.\" There are several famous pizzerias in town, but my two favorites are [Pepe's Pizzeria](https://pepespizzeria.com) and [Modern Apizza](http://modernapizza.com).\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><img src=\"images/pepes-photo.jpg\" alt=\"Pepe's Pizzeria\"></th>\n",
    "    <th><img src=\"images/modern-photo.png\" alt=\"Modern Apizza\"></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "([Photo credit](https://www.eater.com/2014/7/2/6196371/the-road-to-the-38-ranking-new-havens-best-pizzas))\n",
    "\n",
    "If there's one thing that almost all graduate students love, it's good pizza. I couldn't have survived my PhD without it. But if there's one thing we all need to watch out for, it's calories. When you're sitting in the library or lab all day, you've got to watch what you eat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem #1: Pepe's vs. Modern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Data\n",
    "\n",
    "In 2006 my friends and I ate a lot of pizza. Not that we didn't eat a lot of pizza every year we were in New Haven, but this year was different because we all noticed that our clothes were suddenly a lot tighter. We decided to keep track of how much pizza we were eating and how many calories it was costing us. Almost every day (and sometimes more than once a day), we ate at Pepe's or Modern. To decide where to go, we flipped a coin. Once we got there, we asked our server to bring us any pizza they wanted with less than five toppings. One of my friends worked in nutrition, so she would save a slice and bring it back to her lab for testing with the calorimeter. We then recorded the date, pizzeria, toppings, and calories in a spreadsheet.\n",
    "\n",
    "In 2017, I looked at the data we'd collected..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pizza-data.csv', index_col = 'date', parse_dates = True)\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DataFrame into two groups\n",
    "pepe_df = df[df['pizzeria'] == 'Pepe\\'s']\n",
    "modern_df = df[df['pizzeria'] == 'Modern']\n",
    "\n",
    "# Define variables for later\n",
    "pepe_m = pepe_df['calories'].mean()\n",
    "pepe_std = pepe_df['calories'].std()\n",
    "modern_m = modern_df['calories'].mean()\n",
    "modern_std = modern_df['calories'].std()\n",
    "\n",
    "# Print summary statistics\n",
    "print('Pepe\\'s Pizza Summary\\n')\n",
    "print(pepe_df['calories'].describe())\n",
    "print('\\nModern Pizza Summary\\n')\n",
    "print(modern_df['calories'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it looked like Modern pizza had more calories per slice on average. And both seemed to be spread out equally around that mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "To see how the data looked, I plotted two figures: A boxplot and a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pepe_calories = pepe_df['calories']\n",
    "modern_calories = modern_df['calories']\n",
    "\n",
    "# Set fig layout\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Add boxplot\n",
    "sns.boxplot(x=\"pizzeria\", y=\"calories\", data=df, order=['Pepe\\'s', 'Modern'], ax=ax1)\n",
    "ax1.set_xlabel('Pizzeria')\n",
    "ax1.set_ylabel('Calories per Slice')\n",
    "ax1.set_title('Boxplot of Calorie Counts')\n",
    "\n",
    "# Add overlapping histograms\n",
    "sns.distplot(pepe_calories, color=\"blue\", label=\"Pepe\\'s\", ax=ax2)\n",
    "sns.distplot(modern_calories, color=\"orange\", label=\"Modern\", ax=ax2)\n",
    "ax2.set_xlabel('Calories per Slice')\n",
    "ax2.set_ylabel('Frequency (%)')\n",
    "ax2.set_title('Histogram of Calorie Counts')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and histogram reinforced the impression that Modern pizza had more calories and Pepe's. However, there was also a lot of overlap between the two pizzerias, so it was hard to say that the two calorie counts were truly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "With the descriptive statistics and visualizations in mind, I turned to the numbers. In order to do hypothesis testing I needed to start with a **null hypothesis** (usually written as $H_{0}$) and a **test statistic**. In this case, my null hypothesis was that there was no difference between the calorie counts of Pepe's and Modern pizza, and I would test that hypothesis using the mean calorie count of the slices from each pizzeria as my test statistic.\n",
    "\n",
    "I took two approaches to testing my hypothesis. The first was running a simulation, and the second was running a t-test. \n",
    "\n",
    "### Simulation\n",
    "\n",
    "For my simulation, I combined the Pepe's and Modern calorie counts into one pool. I then randomly divided that pool into two groups. Finally, I compared the difference in the means between those two groups with the difference I had first observed between Pepe's and Modern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_sample(data_1, data_2):\n",
    "    '''Takes two np arrays, permutes them, and returns two shuffled arrays of same size.'''\n",
    "    # Concatenate the arrays\n",
    "    data = np.concatenate((data_1, data_2))\n",
    "    \n",
    "    # Permute the combined array\n",
    "    permuted_data = np.random.RandomState(seed=42).permutation(data)\n",
    "    \n",
    "    # Split permuted array\n",
    "    perm_sample_1 = permuted_data[:len(data_1)]\n",
    "    perm_sample_2 = permuted_data[len(data_1):]\n",
    "    return perm_sample_1, perm_sample_2\n",
    "\n",
    "def draw_perm_reps(data_1, data_2, func, size):\n",
    "    '''Runs reps of function on permuted arrays. Returns array of results.'''\n",
    "    # Ititialize array of replicates\n",
    "    perm_replicates = np.empty(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        # Generate permutation sample\n",
    "        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n",
    "        \n",
    "        # Compute test statistic\n",
    "        perm_replicates[i] - func(perm_sample_1, perm_sample_2)\n",
    "    \n",
    "    return perm_replicates\n",
    "\n",
    "def diff_of_means(data_1, data_2):\n",
    "    '''Calculates the difference between two means.'''\n",
    "    diff = np.mean(data_1) - np.mean(data_2)\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute diff of mean\n",
    "empirical_diff_means = diff_of_means(pepe_calories, modern_calories)\n",
    "\n",
    "# Draw permutation replicates\n",
    "perm_replicates = draw_perm_reps(pepe_calories, modern_calories, diff_of_means, size = 10000)\n",
    "\n",
    "# Compute and print p-value\n",
    "p = np.sum(perm_replicates <= empirical_diff_means) / len(perm_replicates)\n",
    "print('p-value =', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the block of code above several times, you'll see that it returns very small p-values. Sometimes they're so small that `numpy` just calculates them as zero! What this p-value is saying is that the chances of seeing a difference between Pepe's and Modern calorie counts that is this size or greater is very unlikely. This suggests that my null hypothesis is false. In other words, there really was a difference between the two pizzerias. If I was going to cut calories, I'd better stick with Pepe's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation\n",
    "\n",
    "For my calculation, I opted for a two-sided t-test using `scipy`. Before discussing the test itself, I want to point out that there are several assumptions that underlying a t-test:\n",
    "\n",
    "1. *The scale of measurement is continuous*. In this case, it's true. Something you eat or drink can have anywhere between 0 (like a Diet Coke) and... well a lot of calories (like Modern's [Italian Bomb](http://modernapizza.com/menu/apizza-specialties/)).\n",
    "2. *The data is collected from representative and random portion of the population*. This is also true. We randomly chose where to eat, and the pizza toppings were chosen randomly for us. And after {{df.shape[0]}} pizzas, we'd eaten pretty much every toppings combination possible, so we weren't favoring lighter or heavier options.\n",
    "3. *The variance of the samples is homogeneous*. In other words, the calorie counts from Pepe's and Modern are equally spread out around their respective means. One way to test that the variances of two populations are equal is using the Bartlett‚Äôs test, which comes as part of the `scipy` library. So if our null hypothesis is that there is no difference in the respective variances of the Pepe's and Modern calorie counts, what does Bartlett have to say about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print('Pepe\\'s variance:', np.round(np.var(pepe_calories), 2))\n",
    "print('Modern variance:', np.round(np.var(modern_calories), 2))\n",
    "var_test = stats.bartlett(pepe_calories, modern_calories)\n",
    "print(var_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a p-value of {{var_test[1].round(3)}}, I failed to reject the null hypothesis. That meant that I had to assume that Pepe's and Modern shared the same variance.\n",
    "\n",
    "One last assumption that's often mentioned for the t-test is that the data should be normally distributed. However, the t-test can be used for non-normally distributed data when there's a lot of data (more than 30 in each group), so wasn't an issue here. My friends and I didn't eat all that pizza for nothing! \n",
    "\n",
    "So with all my assumptions checked, I moved on to my t-test. As with my simulation above, the null hypothesis was that there was no difference between the calorie counts of Pepe's and Modern pizza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.ttest_ind(pepe_calories, modern_calories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the p-value is so small that python is showing it to using using scientific notation. This means that the chance of seeing a difference between the two calorie counts that is this as big or bigger than the one we'd observed was virtually impossible. So, I rejected reject the null hypothesis and concluded that Modern pizza did have more calories than Pepe's. I knew there was a reason it tasted so good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Precision vs. Recall\n",
    "\n",
    "With all the data my friends and I collected, I wondered if I could put it to good use. What I wanted to do was be able to predict whether a slice of pizza came from Pepe's or Modern based only on its number of calories. \n",
    "\n",
    "The challenge of creating a good \"Pizza Predictor\" touches on the question of how p-values relate to precision and recall. Before I discuss my prediction effort, what are precision and recall? Prediction is rarely perfect, and I knew that there would be some slices I'd predict as belonging to Pepe's when they were actually from Modern and vice versa. So my predictions would fall into four categories:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><img src=\"images/confusion-matrix-example.png\" alt=\"Confusion Matrix\"></th>\n",
    "    <th><img src=\"images/confusion-matrix-example-precision.png\" alt=\"Precision\"></th>\n",
    "    <th><img src=\"images/confusion-matrix-example-recall.png\" alt=\"Recall\"></th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "This figure is a great way to visualize precision and recall, which I've highlighted above.\n",
    "\n",
    "* **Precision** focuses on the slices I *predict* as belonging to Pepe's, regardless of whether I'm right or wrong. It's a measure of how well my prediction technique identifies possible slices of Pepe's pizza. Precision keeps me from cheating by only identifying the 1 slice I'm most sure comes from Pepe's. Yes, I'd be right in 100% of my guesses, but there are 99 slices I would've missed.\n",
    "* **Recall** focuses on the slices that are *actually from Pepe's*, regardless of whether I correctly say they are from Pepe's or incorrectly identify them as Modern. It's a measure of how well my technique identifies Pepe's pizza correctly. Recall keeps me from cheating by saying that all 200 slices come from Pepe's. Yes, I'd have identified all the Pepe's pizza correctly, but only by missing all of the Modern pizza.\n",
    "\n",
    "For my predictive model. As above, I assumed that the calorie counts for both brands of pizza followed a normal distribution. I used that assumption to create a smoothed version of the distributions that I'd plotted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "# Set fig layout\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey = True, sharex = True)\n",
    "\n",
    "# Add overlapping histograms\n",
    "sns.distplot(pepe_calories, color=\"blue\", label=\"Pepe\\'s\", ax=ax1)\n",
    "sns.distplot(modern_calories, color=\"orange\", label=\"Modern\", ax=ax1)\n",
    "ax1.set_xlabel('Calories per Slice')\n",
    "ax1.set_ylabel('Frequency (%)')\n",
    "ax1.set_title('Frequency of Calorie Counts')\n",
    "ax1.legend()\n",
    "\n",
    "# Add smoothed distributions\n",
    "sns.distplot(pepe_calories, fit_kws={\"color\":\"blue\"}, kde=False, fit=stats.norm, hist=None, label=\"Pepe's\", ax=ax2)\n",
    "sns.distplot(modern_calories, fit_kws={\"color\":\"orange\"}, kde=False, fit=stats.norm, hist=None, label=\"Modern\", ax=ax2)\n",
    "ax2.set_title('Smoothed Normal Distribution of \\nCalorie Counts')\n",
    "ax2.set_xlabel('Calories per Slice')\n",
    "ax2.set_ylabel('Frequency (%)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I had a pretty picture to help with my predictions but, as with the original, there was a lot over overlap between Pepe's and Modern, which made it hard to identify the slices in the middle of the calorie range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "# For the shading in this plot to work, I had to use Matplotlib directly instead of Seaborn\n",
    "\n",
    "# Fit lines\n",
    "pepe_params = stats.norm.fit(pepe_calories)\n",
    "modern_params = stats.norm.fit(modern_calories)\n",
    "\n",
    "# Set x-axis values\n",
    "xmin = min(pepe_calories.min(), modern_calories.min())\n",
    "xmax = max(pepe_calories.max(), modern_calories.max())\n",
    "x = np.linspace(xmin-50, xmax+50, 100)\n",
    "\n",
    "# Set y-axis values\n",
    "y1 = stats.norm.pdf(x, *stats.norm.fit(pepe_calories))\n",
    "y2 = stats.norm.pdf(x, *stats.norm.fit(modern_calories))\n",
    "\n",
    "# Calculate PDF\n",
    "pepe_pdf = stats.norm(*pepe_params).pdf(x)\n",
    "modern_pdf = stats.norm(*modern_params).pdf(x)\n",
    "\n",
    "# Get y-axis min\n",
    "y = np.minimum(modern_pdf, pepe_pdf)\n",
    "\n",
    "# Calculate overlap\n",
    "cross = x[y1-y2 <= 0][0]\n",
    "\n",
    "# Set layout\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot lines\n",
    "ax.plot(x, pepe_pdf, label=\"Pepe's\", color='blue')\n",
    "ax.plot(x, modern_pdf, label=\"Modern\", color='orange')\n",
    "\n",
    "# Add shading\n",
    "ax.fill_between(x,y1,y2, where=(x<=cross), color=\"blue\", alpha=0.3)\n",
    "ax.fill_between(x,y1,y2, where=(x>=cross), color=\"orange\", alpha=0.3)\n",
    "ax.fill_between(x, y, color='purple', alpha=0.5)\n",
    "\n",
    "# Add legend\n",
    "import matplotlib.patches as mpatches\n",
    "blue_patch = mpatches.Patch(color='blue', alpha=0.3,label='Pepe\\'s')\n",
    "orange_patch = mpatches.Patch(color='orange', alpha=0.3,label='Modern')\n",
    "purple_patch = mpatches.Patch(color='purple', alpha=0.5, label='\"Beats me!\"')\n",
    "plt.legend(handles = [blue_patch, orange_patch, purple_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with this uncertainty, I decided to set a p-value as a threshold or cut-off point. Since I knew that Modern pizza had more calories, I focused on the higher end of the Pepe's distribution and chose a one-sided p-value of 0.05. With my threshold set, I plotted my Pizza Predictor and evaluated its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cut_off(pepe_m, pepe_std, percentile, one_sided = True):\n",
    "    '''Takes p-value of Pepe's distribution and returns corresponding x-axis value for cut-off.'''\n",
    "    if one_sided == True:\n",
    "        n_std = stats.norm.ppf(1 - percentile)\n",
    "    else:\n",
    "        n_std = stats.norm.ppf(1 - percentile/2)\n",
    "    pepe_cut_off = pepe_m + pepe_std * n_std\n",
    "    return pepe_cut_off\n",
    "\n",
    "def predict(cut_off, x):\n",
    "    '''Takes cut-off and returns predicted label.'''\n",
    "    if x <= cut_off:\n",
    "        return 'Pepe\\'s'\n",
    "    else:\n",
    "        return 'Modern'\n",
    "\n",
    "def pepe_confusion(cut_off):\n",
    "    '''Take cut-off and returns confusion matrix.'''\n",
    "    df['prediction'] = df['calories'].apply(lambda x: predict(cut_off, x))\n",
    "    y_true = df['pizzeria'].tolist()\n",
    "    y_pred = df['prediction'].tolist()\n",
    "    return confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set p-value\n",
    "p_value = 0.05\n",
    "\n",
    "# Calculate distance of p-value from mean?\n",
    "n_std = stats.norm.ppf(1 - p_value)\n",
    "\n",
    "# Calculate cut-off and confusion matrix for axes 1 & 2\n",
    "pepe_cut_off = get_cut_off(pepe_m, pepe_std, p_value)\n",
    "cm = pepe_confusion(pepe_cut_off)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Layout\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharex = False, sharey = False)\n",
    "\n",
    "# AX 1: PREDICTOR PLOT\n",
    "sns.distplot(pepe_calories, fit_kws={\"color\":\"grey\"}, kde=False, fit=stats.norm, hist=None, label=\"Pepe's\", ax = ax1)\n",
    "sns.distplot(modern_calories, fit_kws={\"color\":\"grey\"}, kde=False, fit=stats.norm, hist=None, label=\"Modern\", ax = ax1)\n",
    "\n",
    "## Calculate coordinates for vertical line and shading\n",
    "l1 = ax.lines[0]\n",
    "l2 = ax.lines[1]\n",
    "x1 = l1.get_xydata()[:,0]\n",
    "x2 = l2.get_xydata()[:,0]\n",
    "x1min = np.min(x1)\n",
    "x2max = np.max(x2)\n",
    "\n",
    "## Add vertical line and shading\n",
    "ax1.axvline(x = pepe_cut_off, color = 'black')\n",
    "ax1.axvspan(x1min, pepe_cut_off, alpha=0.3, color='blue')\n",
    "ax1.axvspan(pepe_cut_off, x2max, alpha=0.3, color='orange')\n",
    "\n",
    "## Add axis and plot titles\n",
    "ax1.set_xlabel('Calorie Count')\n",
    "ax.set_ylabel('Frequency (%)')\n",
    "ax1.set_title('One-sided p-value = {p}, $\\sigma\\cdot{s}$, cut-off {c}'.format(p = p_value, s = n_std.round(2), c=pepe_cut_off.round()))\n",
    "\n",
    "## Add legend\n",
    "blue_patch = mpatches.Patch(color='blue', alpha=0.3, label='Pepe\\'s')\n",
    "orange_patch = mpatches.Patch(color='orange', alpha=0.3, label='Modern')\n",
    "ax1.legend(handles = [blue_patch, orange_patch])\n",
    "\n",
    "# AX 2: CONFUSION MATRIX\n",
    "ax2.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "\n",
    "## Add counts to figure\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax2.text(j-0.15,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "\n",
    "## Set formatting\n",
    "class_names = ['Modern', 'Pepe\\'s']\n",
    "tick_marks = np.arange(len(class_names))\n",
    "\n",
    "ax2.set_xlabel('Predicted Pizzeria')\n",
    "ax2.set_ylabel('True Pizzeria')\n",
    "ax2.set_xticks(tick_marks)\n",
    "ax2.set_xticklabels(class_names)\n",
    "ax2.set_yticks(tick_marks)\n",
    "ax2.set_yticklabels(class_names)\n",
    "ax2.set_title('Confusion Matrix')\n",
    "\n",
    "# Set figure title\n",
    "plt.suptitle('Pizza Predictor Performance', size = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the numbers in the confusion matrix, I calculated precision and recall. I also added accuracy, the percentage of slices that were correctly identified as either Pepe's or Modern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign values in confusion matrix to variables\n",
    "s = [['tn','fp'], ['fn', 'tp']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        exec(\"{s} = {cm}\".format(s=s[i][j], cm=cm[i][j]))\n",
    "\n",
    "# Calculate precision, recall, accuracy\n",
    "precision = tp / (tp + fp)\n",
    "recall    = tp / (tp + fn)\n",
    "accuracy  = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Print results\n",
    "print('precision :', np.round(precision, decimals = 3))\n",
    "print('recall    :', recall)\n",
    "print('accuracy  :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers showed that my first attempt was not great. I wasn't going to win any awards for {{np.round(precision * 100, 1)}}% accuracy. I had good recall, but my precision was even worse than my accuracy. \n",
    "\n",
    "I order to improve my Pizza Predictor, I had to choose another p-value. But instead of plugging in values one-by-one into my model, I plotted a range of p-values so that I could see how precision, recall, and accuracy changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of p-values\n",
    "p_value_list = np.linspace(.99, 0.01, num = 99).round(2)\n",
    "\n",
    "# Calculate corresponding x-axis cut-off values\n",
    "cut_off_list = []\n",
    "for p in p_value_list:\n",
    "    cut_off = get_cut_off(pepe_m, pepe_std, p)\n",
    "    cut_off_list.append(cut_off)\n",
    "\n",
    "# Add p-values and cut-offs to DataFrame prec_rec\n",
    "prec_rec = pd.DataFrame()\n",
    "prec_rec['p-value'] = p_value_list\n",
    "prec_rec['cut-off'] = cut_off_list\n",
    "\n",
    "# Calulate precision, recall, and accuracy for all p-values, add to prec_rec\n",
    "for lab, row in prec_rec.iterrows():\n",
    "    cm = pepe_confusion(prec_rec.loc[lab, 'cut-off'])\n",
    "    s = [['tn','fp'], ['fn', 'tp']]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            exec(\"{s} = {cm}\".format(s=s[i][j], cm=cm[i][j]))\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = np.nan\n",
    "    recall    = tp / (tp + fn)\n",
    "    accuracy  = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    prec_rec.loc[lab, 'precision'] = precision\n",
    "    prec_rec.loc[lab, 'recall'] = recall\n",
    "    prec_rec.loc[lab, 'accuracy'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "    \n",
    "# Layout\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot precision, recall, and accuracy\n",
    "sns.lineplot(x='p-value', y='precision', data=prec_rec, label='precision', color = 'orange', ax = ax)\n",
    "sns.lineplot(x='p-value', y='recall', data=prec_rec, label='recall', color = 'blue', ax = ax)\n",
    "sns.lineplot(x='p-value', y='accuracy', data=prec_rec, label='accuracy',color = 'green', ax = ax)\n",
    "\n",
    "# Add vertical line at highest accuracy\n",
    "max_acc_y_index = prec_rec.index.get_loc(prec_rec['accuracy'].idxmax())\n",
    "max_acc_x = prec_rec.loc[max_acc_y_index, 'p-value']\n",
    "ax.axvline(x = max_acc_x, color = 'black', linestyle = ':', label = 'best accuracy')\n",
    "max_acc_c = get_cut_off(pepe_m, pepe_std, max_acc_x).round(1)\n",
    "# Formatting\n",
    "ax.set_xlabel('p-value')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "plt.title('Pizza Predictor Performance\\nBest Accuracy: {a}% at p-value {p}\\n({c} Calories)'.format(a = max_acc_y_index, p=max_acc_x, c=max_acc_c))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My plot showed that, as the p-value went up, the precision increases and the recall decreased. I had the best accuracy when the p-value was  {{max_acc_x}}, approximately where precision and recall were equal. So, as a general rule, if a slice had less than {{max_acc_c}} calories, I could say it was Pepe's. If it had more, better to say it was Modern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem #2: Mystery Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pizzeria to Predict Calories\n",
    "\n",
    "I had to face facts: My prediction tool wasn't very good. But not all was lost. The data my friends and I collected was still useful when it came to watching our calories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "# Layout\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey = True, sharex = True)\n",
    "\n",
    "# Set legend colors\n",
    "blue_patch = mpatches.Patch(color='blue', alpha=0.3, label='95% of Slices')\n",
    "red_patch = mpatches.Patch(color='red', alpha=0.3, label='5% of Slices')\n",
    "\n",
    "# AX 1: PEPE'S DISTRIBUTION\n",
    "sns.distplot(pepe_calories, fit_kws={\"color\":\"blue\"}, kde=False, fit=stats.norm, hist=None, label=\"Pepe's\", ax = ax1)\n",
    "\n",
    "## Get coordinates for shading\n",
    "l1 = ax1.lines[0]\n",
    "x1 = l1.get_xydata()[:,0]\n",
    "y1 = l1.get_xydata()[:,1]\n",
    "\n",
    "## Calculate upper and lower bounds\n",
    "ci_upper1 = round(pepe_m + pepe_std * 2)\n",
    "ci_lower1 = round(pepe_m - pepe_std * 2)\n",
    "\n",
    "## Add shading\n",
    "ax1.fill_between(x1,y1, where = ((x1<=ci_lower1)|(x1>=ci_upper1)), color='red', alpha=0.3)\n",
    "ax1.fill_between(x1,y1, where = ((x1>=ci_lower1)&(x1<=ci_upper1)), color='blue', alpha=0.3)\n",
    "\n",
    "## Formatting\n",
    "ax1.legend(handles = [blue_patch, red_patch])\n",
    "ax1.set_xlabel('Calories')\n",
    "ax1.set_ylabel('Frequency (%)')\n",
    "ax1.set_title('Distribution of Pepe\\'s Calorie Counts with\\n95% Confidence Interval: ({l}, {u})'.format(l=ci_lower1, u=ci_upper1))\n",
    "\n",
    "# AX 2: MODERN DISTRIBUTION\n",
    "sns.distplot(modern_calories, fit_kws={\"color\":\"blue\"}, kde=False, fit=stats.norm, hist=None, label=\"Modern\", ax = ax2)\n",
    "\n",
    "## Get coordinates for shading\n",
    "l2 = ax2.lines[0]\n",
    "x2 = l2.get_xydata()[:,0]\n",
    "y2 = l2.get_xydata()[:,1]\n",
    "\n",
    "## Calculate upper and lower bounds\n",
    "ci_upper2 = round(modern_m + modern_std * 2)\n",
    "ci_lower2 = round(modern_m - modern_std * 2)\n",
    "\n",
    "# Add Shading\n",
    "ax2.fill_between(x2,y2, where = ((x2<=ci_lower2)|(x2>=ci_upper2)), color='red', alpha=0.3)\n",
    "ax2.fill_between(x2,y2, where = ((x2>=ci_lower2)&(x2<=ci_upper2)), color='blue', alpha=0.3)\n",
    "\n",
    "## Formatting\n",
    "ax2.legend(handles = [blue_patch, red_patch])\n",
    "ax2.set_xlabel('Calories')\n",
    "ax2.set_ylabel('Frequency (%)')\n",
    "ax2.set_title('Distribution of Modern Calorie Counts with\\n95% Confidence Interval: ({l}, {u})'.format(l=ci_lower2, u=ci_upper2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take for example the above figures. When I eat at Pepe's Pizza, I know my slice will have between {{ci_lower1}} and {{ci_upper1}} calories  95% of the time. For Modern, a slice will have between {{ci_lower2}} and {{ci_upper2}}.\n",
    "\n",
    "Similarly, knowing which pizzeria I'm in is also useful when I know how many calories are in a slice. If I'm eating at Pepe's and I know my slice has 325 calories, I also know what the probability of getting a slice that has that many calories or more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set calories\n",
    "calories = 325\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Set layout\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot Pepe's distribution\n",
    "sns.distplot(pepe_calories, fit_kws={\"color\":\"blue\"}, kde=False, fit=stats.norm, hist=None, ax=ax)\n",
    "\n",
    "# Get coordinates for shading\n",
    "l1 = ax.lines[0]\n",
    "x1 = l1.get_xydata()[:,0]\n",
    "y1 = l1.get_xydata()[:,1]\n",
    "\n",
    "# Add vertical line\n",
    "ax.axvline(x = calories, color = 'black', linestyle = ':', label = '{c} calories'.format(c=calories))\n",
    "\n",
    "# Add shading\n",
    "ax.fill_between(x1,y1, where = (x1<=calories), color='red', alpha=0.3)\n",
    "ax.fill_between(x1,y1, where = ((x1>=calories)), color='blue', alpha=0.3)\n",
    "\n",
    "# Calculate mu and sigma\n",
    "mu, sigma = stats.norm.fit(pepe_calories)[0], stats.norm.fit(pepe_calories)[1]\n",
    "\n",
    "# Use mu and sigma to create labels for legend\n",
    "blue = stats.norm(mu, sigma).cdf(calories) * 100\n",
    "red = 100 - blue\n",
    "\n",
    "# Add legend\n",
    "blue_patch = mpatches.Patch(color='blue', alpha=0.3, label='{r}% of Slices'.format(r=red.round()))\n",
    "red_patch = mpatches.Patch(color='red', alpha=0.3, label='{b}% of Slices'.format(b=blue.round()))\n",
    "plt.legend(handles = [red_patch, blue_patch])\n",
    "\n",
    "# Add label for vertical\n",
    "plt.text(calories + 2, 0.009, '{c} calories'.format(c=calories))\n",
    "\n",
    "plt.title('Distribution of Pepe\\'s Calorie Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted to represent the above figure with an equation, it would look like this: \n",
    "\n",
    "$0.15 = P(üçï \\geq 325 | \\mbox{Pepe's})$\n",
    "\n",
    "Here's the translation in plain English: *\"Given that I'm eating at Pepe's, the probability that my slice has 325 or more calories is 15%.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Calories to Predict Pizzeria\n",
    "\n",
    "The only problem with the probability I figured out above is that it doesn't work in reverse. In other words, if I was eating a slice and all I knew was that it had 325 calories, how could I predict which pizzeria had made it? That equation would look like this:\n",
    "\n",
    "$ü§î = P(\\mbox{Pepe's}|üçï=325)$\n",
    "\n",
    "Or, again, in English: *\"Given that my slice has 325 calories, what's the probability that it came from Pepe's?\"* (I'm only mentioning Pepe's because if I know it doesn't come from Pepe's then I can be sure it came from Modern.)\n",
    "\n",
    "This is precisely the question that I ran into when I went to a happy hour event at my friend's lab. Her advisor had bought 15 pizzas and served them to the group on trays. With no pizza boxes visible, we couldn't know whether the pizza had come from Pepe's or Modern. Luckily, we still had access to the calorimeter and could find out the calorie counts.\n",
    "\n",
    "The way to figure out this problem is with Bayesian inference, which allows you answer these sorts of \"reverse\" questions by using information you already know about the data. The process has three steps:  \n",
    "\n",
    "1. Start with a **prior**: A probability distribution representing what you know before seeing the new data. \n",
    "2. Choose a **generative model**: Your best guess about the character of the new data.\n",
    "3. Interpret your **posterior**: A probability distribution representing what you know after seeing the new data.\n",
    "\n",
    "A quick side note before I go to the pizza. A core principle to Bayesian statistics is that a mean exists as a range of possibilities rather than being a fixed point. In the hypothesis testing I did above, there's an assumption that there's a true mean for the Pepe's and Modern calorie counts and, the more data I collect, the more it will converge on those two means. But with Bayes, those means will move between a range of values as I collect more data. I'll never get to a single point; I'll just keep updating my prior. Very philosophical, I know!\n",
    "\n",
    "OK, back to the mystery pizza problem. I needed to start with some sort of prior. In this case, I already had some information to inform my prior: I knew that a pizza slice ‚Äî whether it was from Pepe's or Modern ‚Äî had between {{df['calories'].min()}} and {{df['calories'].max()}} calories. If I gave myself some padding {{padding = 200}} (say, {{padding}} calories on either side), I felt confident in assuming that all slices of pizza had between {{df['calories'].min() - padding}} and {{df['calories'].max() + padding}} calories. (By the way, this is called an *informed prior* in Bayesian statistics.) I then used that assumption to create a uniform  distribution for my prior, which is a fancy way of saying that I started under the assumption that all slices of pizza were equally likely to contain between {{df['calories'].min() - 150}} and {{df['calories'].max() + 150}} calories.\n",
    "\n",
    "I based the prior for the standard deviation of pizza calories on a Gamma distribution. Gamma distributions are often used to as priors for standard deviations because they are always positive. In contrast to the mean, this is an *uninformed prior*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Layout\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# AX 1: MEAN PRIOR\n",
    "\n",
    "## Plot Pepe's and Modern calorie distributions for comparison\n",
    "sns.distplot(pepe_calories, fit_kws={\"color\":\"grey\"}, kde=False, fit=stats.norm, hist=None, label=\"Pepe's\", ax=ax1)\n",
    "sns.distplot(modern_calories, fit_kws={\"color\":\"grey\"}, kde=False, fit=stats.norm, hist=None, label=\"Modern\", ax=ax1)\n",
    "\n",
    "## Plot prior distribution\n",
    "padding = 200\n",
    "prior_mu_min = df['calories'].min() - padding\n",
    "prior_mu_max =  df['calories'].max() + padding\n",
    "uniform = patches.Rectangle((prior_mu_min, 0), prior_mu_max - prior_mu_min, 0.005, linewidth=1.0, facecolor = 'blue', alpha = 0.3)\n",
    "ax1.add_patch(uniform)\n",
    "\n",
    "## Add legend\n",
    "blue_patch = mpatches.Patch(color='blue', alpha=0.3, label='Prior Distribution')\n",
    "grey_patch = mpatches.Patch(color='grey', alpha=0.3, label='Pepe\\'s & Modern Data')\n",
    "ax1.legend(handles = [grey_patch, blue_patch])\n",
    "\n",
    "## Formatting\n",
    "ax1.set_xlabel('Calories')\n",
    "ax1.set_ylabel('Frequency (%)')\n",
    "ax1.set_title('Uniform Distribution of Mean')\n",
    "\n",
    "# AX 2: STD PRIOR DISTRIBUTION\n",
    "\n",
    "## Set alpha and beta\n",
    "alpha = 0.001\n",
    "beta = 0.001\n",
    "\n",
    "## Plot distribution\n",
    "x = np.linspace(0, 1, 100) \n",
    "y1 = stats.gamma.pdf(x, a=alpha, scale=1/beta)\n",
    "ax2.plot(x, y1, color = 'blue', alpha = 0.5, label=(r'$\\alpha={a}, \\beta={b}$'.format(a=alpha, b=beta)))\n",
    "\n",
    "## Legend and formatting\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Standard Deviation of Mean Calories')\n",
    "ax2.set_ylabel('Frequency (%)')\n",
    "ax2.set_title('Gamma Distribution of SD')\n",
    "\n",
    "plt.suptitle('Priors for Mystery Pizza Calorie Count', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my generative model, I assumed that the calorie counts for the mystery pizza were distributed normally (like I had done with the hypothesis test).\n",
    "\n",
    "With all those assumptions in place, I measured the calories for each of the 15 slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data, print info\n",
    "mystery_df = pd.read_csv('mystery-pizza-data.csv')\n",
    "print(mystery_df.info())\n",
    "mystery_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I used my new data to \"update\" my prior and create a posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "# Set parameters for prior\n",
    "prior_sigma_min = 0\n",
    "prior_sigma_max = prior_sigma_min + df['calories'].std() * 2\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Define priors\n",
    "    mystery_mu = pm.Uniform('Mystery Pizza Mean', lower = 0, upper = 1000)\n",
    "    #mystery_sigma = pm.Uniform('Mystery Pizza SD', lower = prior_sigma_min, upper = prior_sigma_max)\n",
    "    mystery_sigma = pm.Gamma('Mystery Pizza SD', 0.001, 0.001)\n",
    "    \n",
    "    # Define likelihood (generative model)\n",
    "    mystery_pizza = pm.Normal('mystery', mu=mystery_mu, sd=mystery_sigma, observed=mystery_df['calories'].values)\n",
    "    \n",
    "    # Inference (update priors to get posterior)\n",
    "    trace = pm.sample(2000, cores=2, tune=2500)\n",
    "\n",
    "# NOTE: If you're running this notebook on Binder, this cell will take some time to run.\n",
    "# And you can ignore these warnings:\n",
    "# WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
    "# WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "varnames=['Mystery Pizza Mean','Mystery Pizza SD']\n",
    "pm.plot_posterior(trace, varnames=varnames)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "mystery_p_mean = trace['Mystery Pizza Mean']\n",
    "mystery_p_std  = trace['Mystery Pizza SD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the posterior distributions, I knew that the mean calorie count of my mystery pizza was likely between {{np.round(mystery_p_mean.mean() - mystery_p_mean.std() * 2).astype('int')}} and {{np.round(mystery_p_mean.mean() + mystery_p_mean.std() * 2).astype('int')}}, and the standard deviation was between {{np.round(mystery_p_std.mean() - mystery_p_std.std() * 2).astype('int')}} and {{np.round(mystery_p_std.mean() + mystery_p_std.std() * 2).astype('int')}}. So I plotted the range of possibilities in a heat map, where I also included the information I already knew about Pepe's and Modern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "# Set parameters\n",
    "n_bins = 30\n",
    "heatmap, xedges, yedges = np.histogram2d(mystery_p_mean, mystery_p_std, bins=n_bins)\n",
    "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.imshow(heatmap.T, extent=extent, origin='lower', aspect ='auto', cmap = plt.cm.Greys)\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot prior Pepe's and Modern data\n",
    "plt.scatter(pepe_calories.mean(), pepe_calories.std(), s=200, c='cyan', marker='.')\n",
    "plt.scatter(modern_calories.mean(), modern_calories.std(), s=200, c='orange', marker='.')\n",
    "\n",
    "# Add legend\n",
    "blue_patch = mpatches.Patch(facecolor='cyan', label='Pepe\\'s', edgecolor = 'black')\n",
    "orange_patch = mpatches.Patch(facecolor='orange', label='Modern', edgecolor = 'black')\n",
    "plt.legend(handles = [blue_patch, orange_patch])\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Calories, Mean')\n",
    "plt.ylabel('Calories, SD')\n",
    "plt.title('Mystery Pizza Heatmap')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also calculated the distance between the black cluster and both Pepe's and Modern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_distance (m_mean, m_std, series):\n",
    "    '''Takes arrays of mystery pizza mean and std, and series of calorie counts.\n",
    "    Calculates distance between (1) mean pizza mean and std, and (2) calorie count mean and std.'''\n",
    "    m_mean = m_mean.mean()\n",
    "    m_std = m_std.mean()\n",
    "    k_mean = series.mean()\n",
    "    k_std = series.std()\n",
    "    dist = math.hypot(m_mean - k_mean, m_std - k_std)\n",
    "    return dist\n",
    "\n",
    "def compare_distances(m_mean, m_std, pepe, modern):\n",
    "    '''Takes arrays of (1) mystery pizza mean and (2) std; series for (1) Pepe's and (2) Modern calorie counts.\n",
    "    Calculates whether Pepe's or Modern is closer.'''\n",
    "    dist_pepe = get_distance(m_mean, m_std, pepe)\n",
    "    dist_modern = get_distance(m_mean, m_std, modern)\n",
    "    if dist_pepe < dist_modern:\n",
    "        response = 'Pepe\\'s was closer'\n",
    "    elif dist_pepe > dist_modern:\n",
    "        response = 'Modern was closer'\n",
    "    else:\n",
    "        reponse = 'Pepe\\'s and Modern were equally close'\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_pizzeria = compare_distances(m_mean = mystery_p_mean, \n",
    "                                     m_std = mystery_p_std, \n",
    "                                     pepe = pepe_calories, \n",
    "                                     modern = modern_calories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the heatmap and my calculation, it was clear that {{closest_pizzeria}}. So the answer was solved. Based on prior knowledge we had gained from the hundreds of slices we'd eaten, we were able to determine that the 15 new slices had come that pizzeria. Mystery solved thanks to Bayesian inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
